{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hoang\\OneDrive\\Desktop\\EDABK_YOLOV7_TINY_WEAPON_DETECTION\\yolov7\n"
     ]
    }
   ],
   "source": [
    "%cd yolov7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOR  c37858a torch 1.13.0+cpu CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "IDetect.fuse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 314 layers, 36492560 parameters, 6194944 gradients, 103.2 GFLOPS\n"
     ]
    }
   ],
   "source": [
    "# Import thư viện để nhận diện ảnh trong thực tế\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "from numpy import random\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import LoadStreams, LoadImages\n",
    "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
    "    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
    "from utils.plots import plot_one_box\n",
    "from utils.torch_utils import select_device, load_classifier, time_synchronized, TracedModel\n",
    "\n",
    "def letterbox(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = img.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better test mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    ratio = r, r  # width, height ratios\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "    elif scaleFill:  # stretch\n",
    "        dw, dh = 0.0, 0.0\n",
    "        new_unpad = (new_shape[1], new_shape[0])\n",
    "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return img, ratio, (dw, dh)\n",
    "  \n",
    "classes_to_filter = None  #You can give list of classes to filter by name, Be happy you don't have to put class number. ['train','person' ]\n",
    "source_image_path = \"../data/test/images/weapon_142.png\"\n",
    "\n",
    "opt  = {\n",
    "    \n",
    "    \"weights\": \"./runs/train/exp5/weights/best.pt\", # Path to weights file default weights are for nano model\n",
    "    \"yaml\"   : \"./data/dataset.yaml\",\n",
    "    \"img-size\": 640, # default image size\n",
    "    \"conf-thres\": 0.1, # confidence threshold for inference.\n",
    "    \"iou-thres\" : 0.65, # NMS IoU threshold for inference.\n",
    "    \"device\" : 'cpu',  # device to run our model i.e. 0 or 0,1,2,3 or cpu\n",
    "    \"classes\" : classes_to_filter  # list of classes to filter or None\n",
    "\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "  weights, imgsz = opt['weights'], opt['img-size']\n",
    "  set_logging()\n",
    "  device = select_device(opt['device'])\n",
    "  half = device.type != 'cpu'\n",
    "  model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "  stride = int(model.stride.max())  # model stride\n",
    "  imgsz = check_img_size(imgsz, s=stride)  # check img_size\n",
    "  if half:\n",
    "    model.half()\n",
    "\n",
    "  names = model.module.names if hasattr(model, 'module') else model.names\n",
    "  colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
    "  if device.type != 'cpu':\n",
    "    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))\n",
    "\n",
    "  img0 = cv2.imread(source_image_path)\n",
    "  img = letterbox(img0, imgsz, stride=stride)[0]\n",
    "  img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "  img = np.ascontiguousarray(img)\n",
    "  img = torch.from_numpy(img).to(device)\n",
    "  img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "  img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "  if img.ndimension() == 3:\n",
    "    img = img.unsqueeze(0)\n",
    "\n",
    "  # Inference\n",
    "  t1 = time_synchronized()\n",
    "  pred = model(img, augment= False)[0]\n",
    "\n",
    "  # Apply NMS\n",
    "  classes = None\n",
    "  if opt['classes']:\n",
    "    classes = []\n",
    "    for class_name in opt['classes']:\n",
    "\n",
    "      classes.append(opt['classes'].index(class_name))\n",
    "\n",
    "\n",
    "  pred = non_max_suppression(pred, opt['conf-thres'], opt['iou-thres'], classes= classes, agnostic= False)\n",
    "  t2 = time_synchronized()\n",
    "  for i, det in enumerate(pred):\n",
    "    s = ''\n",
    "    s += '%gx%g ' % img.shape[2:]  # print string\n",
    "    gn = torch.tensor(img0.shape)[[1, 0, 1, 0]]\n",
    "    if len(det):\n",
    "      det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round()\n",
    "\n",
    "      for c in det[:, -1].unique():\n",
    "        n = (det[:, -1] == c).sum()  # detections per class\n",
    "        s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
    "    \n",
    "      for *xyxy, conf, cls in reversed(det):\n",
    "\n",
    "        label = f'{names[int(cls)]} {conf:.2f}'\n",
    "        plot_one_box(xyxy, img0, label=label, color=colors[int(cls)], line_thickness=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "cv2.imshow('img', img0)\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('torch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e43541ca39c027704f3e2b5f680b3b742d1a94a7d35145d12ddfdd9d0c0bf48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
